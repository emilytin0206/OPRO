# ----------------------------------------------------
# 專案設定
# ----------------------------------------------------
project:
  task_name: 'BBH-boolean_expressions' # 任務名稱
  log_dir: './logs'                    # 日誌輸出路徑

# ----------------------------------------------------
# Scorer Model (用於評估指令效果)
# ----------------------------------------------------
scorer_model:
  client_type: 'Ollama' # 或 'PaLM'
  model_name: 'llama2'  # Ollama 的模型名稱, 例如 'llama2', 'mistral'
  api_url: 'http://localhost:11434/api/generate' # Ollama API 服務網址
  temperature: 0.0      # 較低溫度以確保確定性評估
  max_output_tokens: 1024

# ----------------------------------------------------
# Optimizer Model (用於生成新指令)
# ----------------------------------------------------
optimizer_model:
  client_type: 'Ollama' # 或 'PaLM'
  model_name: 'codellama' # Ollama 的模型名稱, 例如 'codellama', 'mixtral'
  api_url: 'http://localhost:11434/api/generate' # Ollama API 服務網址
  temperature: 0.5      # 適度溫度以產生多樣化的新指令
  max_output_tokens: 2048

# ----------------------------------------------------
# OPRO 優化參數
# ----------------------------------------------------
optimization:
  num_iterations: 100       # 總迭代次數
  num_evals_per_prompt: 3   # 每個提示詞評估次數 (用於計算平均分數)
  num_prompts_to_generate: 4 # 每次迭代生成的新提示詞數量
  max_num_instructions_in_prompt: 20         # 選取 Top K 最佳提示詞進行優化
  meta_prompt_path: 'prompts/meta_prompt.txt'